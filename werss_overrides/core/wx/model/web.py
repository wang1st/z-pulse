# Override for weRSS web.py to fix pagination issue
# This file fixes the issue where only the first page is crawled

import json
import time
import random
import requests
from datetime import datetime
from core.wx.base import WxGather
from core.print import print_info

class MpsWeb(WxGather):
    """Override MpsWeb class to fix pagination"""
    
    def get_Articles(self, faker_id: str = None, Mps_id: str = None, Mps_title: str = "", 
                     CallBack=None, start_page: int = 0, MaxPage: int = 1, interval: int = 10, 
                     Gather_Content: bool = False, Item_Over_CallBack=None, Over_CallBack=None):
        """é‡å†™ get_Articles æ–¹æ³•ï¼Œä¿®å¤åˆ†é¡µé—®é¢˜"""
        super().Start(mp_id=Mps_id)
        if self.Gather_Content:
            Gather_Content = True
        print(f"Webæµè§ˆå™¨æ¨¡å¼,æ˜¯å¦é‡‡é›†[{Mps_title}]å†…å®¹ï¼š{Gather_Content}\n")
        
        # è¯·æ±‚å‚æ•°
        url = "https://mp.weixin.qq.com/cgi-bin/appmsgpublish"
        count = 5
        params = {
            "sub": "list",
            "sub_action": "list_ex",
            "begin": start_page,
            "count": count,
            "fakeid": faker_id,
            "token": self.token,
            "lang": "zh_CN",
            "f": "json",
            "ajax": 1
        }
        
        session = self.session
        i = start_page
        
        print(f"å¼€å§‹çˆ¬å–ï¼Œèµ·å§‹é¡µ: {start_page}, æœ€å¤§é¡µ: {MaxPage}\n")
        
        while True:
            if i >= MaxPage:
                print(f"å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {MaxPage}ï¼Œåœæ­¢çˆ¬å–\n")
                break
            
            begin = i * count
            params["begin"] = str(begin)
            print(f"ç¬¬{i+1}é¡µå¼€å§‹çˆ¬å–\n")
            
            # éšæœºæš‚åœå‡ ç§’ï¼Œé¿å…è¿‡å¿«çš„è¯·æ±‚å¯¼è‡´è¿‡å¿«çš„è¢«æŸ¥åˆ°
            time.sleep(random.randint(0, interval))
            
            try:
                headers = self.fix_header(url)
                resp = session.get(url, headers=headers, params=params, verify=False, timeout=30)
                
                msg = resp.json()
                self._cookies = resp.cookies
                
                # æµé‡æŽ§åˆ¶äº†, é€€å‡º
                if msg['base_resp']['ret'] == 200013:
                    super().Error("frequencey control, stop at {}".format(str(begin)))
                    break
                
                if msg['base_resp']['ret'] == 200003:
                    super().Error("Invalid Session, stop at {}".format(str(begin)), code="Invalid Session")
                    break
                
                if msg['base_resp']['ret'] != 0:
                    super().Error("é”™è¯¯åŽŸå› :{}:ä»£ç :{}".format(msg['base_resp']['err_msg'], msg['base_resp']['ret']), 
                                 code=msg['base_resp']['err_msg'])
                    break
                
                # å¦‚æžœè¿”å›žçš„å†…å®¹ä¸­ä¸ºç©ºåˆ™ç»“æŸ
                if 'publish_page' not in msg:
                    super().Error("all ariticle parsed")
                    break
                
                if "publish_page" in msg:
                    msg["publish_page"] = json.loads(msg['publish_page'])
                    for item in msg["publish_page"]['publish_list']:
                        if "publish_info" in item:
                            publish_info = json.loads(item['publish_info'])
                            
                            if "appmsgex" in publish_info:
                                for item in publish_info["appmsgex"]:
                                    # è¾“å‡ºæ–‡ç« ä¿¡æ¯æ—¥å¿—
                                    title = item.get('title', 'æœªçŸ¥æ ‡é¢˜')
                                    publish_date_str = "æœªçŸ¥"
                                    
                                    # å°è¯•èŽ·å–å‘å¸ƒæ—¥æœŸï¼ˆitemä¸­å¯èƒ½æœ‰update_timeæˆ–update_etimeå­—æ®µï¼‰
                                    if 'update_time' in item:
                                        try:
                                            # update_time å¯èƒ½æ˜¯æ—¶é—´æˆ³ï¼ˆç§’ï¼‰æˆ–æ—¥æœŸå­—ç¬¦ä¸²
                                            update_time = item['update_time']
                                            if isinstance(update_time, (int, float)):
                                                publish_date = datetime.fromtimestamp(int(update_time))
                                                publish_date_str = publish_date.strftime("%Y-%m-%d %H:%M:%S")
                                            elif isinstance(update_time, str):
                                                publish_date_str = update_time
                                        except:
                                            publish_date_str = str(item.get('update_time', 'æœªçŸ¥'))
                                    elif 'update_etime' in item:
                                        try:
                                            update_etime = item['update_etime']
                                            if isinstance(update_etime, (int, float)):
                                                publish_date = datetime.fromtimestamp(int(update_etime))
                                                publish_date_str = publish_date.strftime("%Y-%m-%d %H:%M:%S")
                                            elif isinstance(update_etime, str):
                                                publish_date_str = update_etime
                                        except:
                                            publish_date_str = str(item.get('update_etime', 'æœªçŸ¥'))
                                    
                                    print_info(f"ðŸ“° æ–‡ç« ä¿¡æ¯ - å…¬ä¼—å·: {Mps_title}, æ ‡é¢˜: {title}, å‘å¸ƒæ—¥æœŸ: {publish_date_str}")
                                    
                                    if Gather_Content:
                                        if not super().HasGathered(item["aid"]):
                                            item["content"] = self.content_extract(item['link'])
                                            super().Wait(3, 10, tips=f"{item['title']} é‡‡é›†å®Œæˆ")
                                    else:
                                        item["content"] = ""
                                    item["id"] = item["aid"]
                                    item["mp_id"] = Mps_id
                                    if CallBack is not None:
                                        super().FillBack(CallBack=CallBack, data=item, 
                                                        Ext_Data={"mp_title": Mps_title, "mp_id": Mps_id})
                    print(f"ç¬¬{i+1}é¡µçˆ¬å–æˆåŠŸ\n")
                    # ç¿»é¡µ - ç¡®ä¿åœ¨æˆåŠŸå¤„ç†åŽé€’å¢ž
                    i += 1
                else:
                    # å¦‚æžœæ²¡æœ‰ publish_pageï¼Œä¹Ÿé€’å¢žå¹¶ç»§ç»­ï¼ˆå¯èƒ½æ˜¯æœ€åŽä¸€é¡µï¼‰
                    print(f"ç¬¬{i+1}é¡µæ— å†…å®¹ï¼Œç»§ç»­ä¸‹ä¸€é¡µ\n")
                    i += 1
                
            except requests.exceptions.Timeout:
                print(f"Request timed out at page {i+1}")
                # è¶…æ—¶åŽä¹Ÿé€’å¢žï¼Œç»§ç»­ä¸‹ä¸€é¡µ
                i += 1
                # å¦‚æžœè¿žç»­è¶…æ—¶å¤šæ¬¡ï¼Œé€€å‡º
                if i >= MaxPage:
                    break
            except requests.exceptions.RequestException as e:
                print(f"Request error at page {i+1}: {e}")
                # è¯·æ±‚é”™è¯¯åŽä¹Ÿé€’å¢žï¼Œç»§ç»­ä¸‹ä¸€é¡µ
                i += 1
                # å¦‚æžœè¿žç»­é”™è¯¯å¤šæ¬¡ï¼Œé€€å‡º
                if i >= MaxPage:
                    break
            except Exception as e:
                print(f"Unexpected error at page {i+1}: {e}")
                # å…¶ä»–é”™è¯¯åŽä¹Ÿé€’å¢žï¼Œç»§ç»­ä¸‹ä¸€é¡µ
                i += 1
                if i >= MaxPage:
                    break
            finally:
                super().Item_Over(item={"mps_id": Mps_id, "mps_title": Mps_title}, 
                                 CallBack=Item_Over_CallBack)
        
        super().Over(CallBack=Over_CallBack)

